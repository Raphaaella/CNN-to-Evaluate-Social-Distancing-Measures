{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image, ImageFile\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import config\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# import helper functions from utils\n",
    "from utils import (\n",
    "    seed_everything,\n",
    "    mean_average_precision,\n",
    "    cells_to_bboxes,\n",
    "    get_evaluation_bboxes,\n",
    "    iou_width_height as iou,\n",
    "    non_max_suppression as nms,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    check_class_accuracy,\n",
    "    get_loaders,\n",
    "    plot_couple_examples,\n",
    "    intersection_over_union\n",
    ")\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Source</th>\n",
       "      <th>LabelName</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>XMin</th>\n",
       "      <th>XMax</th>\n",
       "      <th>YMin</th>\n",
       "      <th>YMax</th>\n",
       "      <th>IsOccluded</th>\n",
       "      <th>IsTruncated</th>\n",
       "      <th>...</th>\n",
       "      <th>IsDepiction</th>\n",
       "      <th>IsInside</th>\n",
       "      <th>XClick1X</th>\n",
       "      <th>XClick2X</th>\n",
       "      <th>XClick3X</th>\n",
       "      <th>XClick4X</th>\n",
       "      <th>XClick1Y</th>\n",
       "      <th>XClick2Y</th>\n",
       "      <th>XClick3Y</th>\n",
       "      <th>XClick4Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>xclick</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.059375</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.357812</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>xclick</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.276563</td>\n",
       "      <td>0.714063</td>\n",
       "      <td>0.948438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.248438</td>\n",
       "      <td>0.276563</td>\n",
       "      <td>0.214062</td>\n",
       "      <td>0.914062</td>\n",
       "      <td>0.714063</td>\n",
       "      <td>0.782813</td>\n",
       "      <td>0.948438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>xclick</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151562</td>\n",
       "      <td>0.310937</td>\n",
       "      <td>0.198437</td>\n",
       "      <td>0.590625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>0.151562</td>\n",
       "      <td>0.310937</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.198437</td>\n",
       "      <td>0.434375</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.590625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>xclick</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.256250</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.651563</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315625</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.256250</td>\n",
       "      <td>0.423438</td>\n",
       "      <td>0.651563</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.826562</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>xclick</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.346875</td>\n",
       "      <td>0.235938</td>\n",
       "      <td>0.385938</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.317188</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.346875</td>\n",
       "      <td>0.307812</td>\n",
       "      <td>0.235938</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>0.348438</td>\n",
       "      <td>0.385938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ImageID  Source  LabelName  Confidence      XMin      XMax  \\\n",
       "0  000002b66c9c498e  xclick  /m/01g317           1  0.012500  0.195312   \n",
       "1  000002b66c9c498e  xclick  /m/01g317           1  0.025000  0.276563   \n",
       "2  000002b66c9c498e  xclick  /m/01g317           1  0.151562  0.310937   \n",
       "3  000002b66c9c498e  xclick  /m/01g317           1  0.256250  0.429688   \n",
       "4  000002b66c9c498e  xclick  /m/01g317           1  0.257812  0.346875   \n",
       "\n",
       "       YMin      YMax  IsOccluded  IsTruncated  ...  IsDepiction  IsInside  \\\n",
       "0  0.148438  0.587500           0            1  ...            0         0   \n",
       "1  0.714063  0.948438           0            1  ...            0         0   \n",
       "2  0.198437  0.590625           1            0  ...            0         0   \n",
       "3  0.651563  0.925000           1            0  ...            0         0   \n",
       "4  0.235938  0.385938           1            0  ...            0         0   \n",
       "\n",
       "   XClick1X  XClick2X  XClick3X  XClick4X  XClick1Y  XClick2Y  XClick3Y  \\\n",
       "0  0.148438  0.012500  0.059375  0.195312  0.148438  0.357812  0.587500   \n",
       "1  0.025000  0.248438  0.276563  0.214062  0.914062  0.714063  0.782813   \n",
       "2  0.243750  0.151562  0.310937  0.262500  0.198437  0.434375  0.507812   \n",
       "3  0.315625  0.429688  0.256250  0.423438  0.651563  0.921875  0.826562   \n",
       "4  0.317188  0.257812  0.346875  0.307812  0.235938  0.289062  0.348438   \n",
       "\n",
       "   XClick4Y  \n",
       "0  0.325000  \n",
       "1  0.948438  \n",
       "2  0.590625  \n",
       "3  0.925000  \n",
       "4  0.385938  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv file with box annotations\n",
    "person_annotations = pd.read_csv('../../02_data/Google_OpenImages/filtered_person_annotations.csv')\n",
    "person_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>LabelName</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>x_center</th>\n",
       "      <th>y_center</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.103906</td>\n",
       "      <td>0.367969</td>\n",
       "      <td>0.182812</td>\n",
       "      <td>0.439062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.150782</td>\n",
       "      <td>0.831251</td>\n",
       "      <td>0.251563</td>\n",
       "      <td>0.234375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.231249</td>\n",
       "      <td>0.394531</td>\n",
       "      <td>0.159375</td>\n",
       "      <td>0.392188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.342969</td>\n",
       "      <td>0.788282</td>\n",
       "      <td>0.173438</td>\n",
       "      <td>0.273437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000002b66c9c498e</td>\n",
       "      <td>/m/01g317</td>\n",
       "      <td>1</td>\n",
       "      <td>0.302343</td>\n",
       "      <td>0.310938</td>\n",
       "      <td>0.089063</td>\n",
       "      <td>0.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ImageID  LabelName  Confidence  x_center  y_center     width  \\\n",
       "0  000002b66c9c498e  /m/01g317           1  0.103906  0.367969  0.182812   \n",
       "1  000002b66c9c498e  /m/01g317           1  0.150782  0.831251  0.251563   \n",
       "2  000002b66c9c498e  /m/01g317           1  0.231249  0.394531  0.159375   \n",
       "3  000002b66c9c498e  /m/01g317           1  0.342969  0.788282  0.173438   \n",
       "4  000002b66c9c498e  /m/01g317           1  0.302343  0.310938  0.089063   \n",
       "\n",
       "     height  \n",
       "0  0.439062  \n",
       "1  0.234375  \n",
       "2  0.392188  \n",
       "3  0.273437  \n",
       "4  0.150000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create necessary columns \n",
    "person_annotations[\"x_center\"] = (person_annotations[\"XMin\"] + person_annotations[\"XMax\"]) / 2\n",
    "person_annotations[\"y_center\"] = (person_annotations[\"YMin\"] + person_annotations[\"YMax\"]) / 2\n",
    "person_annotations[\"width\"] = person_annotations[\"XMax\"] - person_annotations[\"XMin\"]\n",
    "person_annotations[\"height\"] = person_annotations[\"YMax\"] - person_annotations[\"YMin\"]\n",
    "\n",
    "# select only relevant columns\n",
    "person_annotations = person_annotations[[\"ImageID\", \"LabelName\", \"Confidence\", \"x_center\", \"y_center\", \"width\", \"height\"]]\n",
    "\n",
    "# see if the changes worked\n",
    "person_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Information about architecture config:\n",
    "Tuple is structured by (filters, kernel_size, stride) \n",
    "Every conv is a same convolution. \n",
    "List is structured by \"B\" indicating a residual block followed by the number of repeats\n",
    "\"S\" is for scale prediction block and computing the yolo loss\n",
    "\"U\" is for upsampling the feature map and concatenating with a previous layer\n",
    "\"\"\"\n",
    "config = [\n",
    "    (32, 3, 1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\", 1],\n",
    "    (128, 3, 2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4],  # To this point is Darknet-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=80):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []  # for each scale\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2),)\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_classes = 20\n",
    "    IMAGE_SIZE = 416\n",
    "    model = YOLOv3(num_classes=num_classes)\n",
    "    x = torch.randn((2, 3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    out = model(x)\n",
    "    assert model(x)[0].shape == (2, 3, IMAGE_SIZE//32, IMAGE_SIZE//32, num_classes + 5)\n",
    "    assert model(x)[1].shape == (2, 3, IMAGE_SIZE//16, IMAGE_SIZE//16, num_classes + 5)\n",
    "    assert model(x)[2].shape == (2, 3, IMAGE_SIZE//8, IMAGE_SIZE//8, num_classes + 5)\n",
    "    print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Constants signifying how much to pay for each respective part of the loss\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_noobj = 10\n",
    "        self.lambda_obj = 1\n",
    "        self.lambda_box = 10\n",
    "\n",
    "    def forward(self, predictions, target, anchors):\n",
    "        # Check where obj and noobj (we ignore if target == -1)\n",
    "        obj = target[..., 0] == 1  # in paper this is Iobj_i\n",
    "        noobj = target[..., 0] == 0  # in paper this is Inoobj_i\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        no_object_loss = self.bce(\n",
    "            (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
    "        ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "        object_loss = self.mse(self.sigmoid(predictions[..., 0:1][obj]), ious * target[..., 0:1][obj])\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])  # x,y coordinates\n",
    "        target[..., 3:5] = torch.log(\n",
    "            (1e-16 + target[..., 3:5] / anchors)\n",
    "        )  # width, height coordinates\n",
    "        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.entropy(\n",
    "            (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
    "        )\n",
    "\n",
    "        #print(\"__________________________________\")\n",
    "        #print(self.lambda_box * box_loss)\n",
    "        #print(self.lambda_obj * object_loss)\n",
    "        #print(self.lambda_noobj * no_object_loss)\n",
    "        #print(self.lambda_class * class_loss)\n",
    "        #print(\"\\n\")\n",
    "\n",
    "        return (\n",
    "            self.lambda_box * box_loss\n",
    "            + self.lambda_obj * object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + self.lambda_class * class_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ANCHORS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 106\u001b[0m\n\u001b[1;32m    102\u001b[0m         plot_image(x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m), boxes)\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m     test()\n",
      "Cell \u001b[0;32mIn[15], line 73\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m():\n\u001b[0;32m---> 73\u001b[0m     anchors \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mANCHORS\n\u001b[1;32m     75\u001b[0m     transform \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mtest_transforms\n\u001b[1;32m     77\u001b[0m     dataset \u001b[39m=\u001b[39m YOLODataset(\n\u001b[1;32m     78\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCOCO/train.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCOCO/images/images/\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m         transform\u001b[39m=\u001b[39mtransform,\n\u001b[1;32m     84\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ANCHORS'"
     ]
    }
   ],
   "source": [
    "class YOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        img_dir,\n",
    "        label_dir,\n",
    "        anchors,\n",
    "        image_size=416,\n",
    "        S=[13, 26, 52],\n",
    "        C=20,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.C = C\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "            image = augmentations[\"image\"]\n",
    "            bboxes = augmentations[\"bboxes\"]\n",
    "\n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou(torch.tensor(box[2:4]), self.anchors)\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, width, height, class_label = box\n",
    "            has_anchor = [False] * 3  # each scale should have one anchor\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                S = self.S[scale_idx]\n",
    "                i, j = int(S * y), int(S * x)  # which cell\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_cell, y_cell = S * x - j, S * y - i  # both between [0,1]\n",
    "                    width_cell, height_cell = (\n",
    "                        width * S,\n",
    "                        height * S,\n",
    "                    )  # can be greater than 1 since it's relative to cell\n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
    "\n",
    "        return image, tuple(targets)\n",
    "\n",
    "\n",
    "def test():\n",
    "    anchors = config.ANCHORS\n",
    "\n",
    "    transform = config.test_transforms\n",
    "\n",
    "    dataset = YOLODataset(\n",
    "        \"COCO/train.csv\",\n",
    "        \"COCO/images/images/\",\n",
    "        \"COCO/labels/labels_new/\",\n",
    "        S=[13, 26, 52],\n",
    "        anchors=anchors,\n",
    "        transform=transform,\n",
    "    )\n",
    "    S = [13, 26, 52]\n",
    "    scaled_anchors = torch.tensor(anchors) / (\n",
    "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    )\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "    for x, y in loader:\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(y[0].shape[1]):\n",
    "            anchor = scaled_anchors[i]\n",
    "            print(anchor.shape)\n",
    "            print(y[i].shape)\n",
    "            boxes += cells_to_bboxes(\n",
    "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
    "            )[0]\n",
    "        boxes = nms(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
    "        print(boxes)\n",
    "        plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for InitSchema\n  Value error, If 'border_mode' is set to 'BORDER_CONSTANT', 'value' must be provided. [type=value_error, input_value={'min_height': 457, 'min_..._apply': None, 'p': 1.0}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 33\u001b[0m\n\u001b[1;32m     22\u001b[0m ANCHORS \u001b[39m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m     [(\u001b[39m0.28\u001b[39m, \u001b[39m0.22\u001b[39m), (\u001b[39m0.38\u001b[39m, \u001b[39m0.48\u001b[39m), (\u001b[39m0.9\u001b[39m, \u001b[39m0.78\u001b[39m)],\n\u001b[1;32m     24\u001b[0m     [(\u001b[39m0.07\u001b[39m, \u001b[39m0.15\u001b[39m), (\u001b[39m0.15\u001b[39m, \u001b[39m0.11\u001b[39m), (\u001b[39m0.14\u001b[39m, \u001b[39m0.29\u001b[39m)],\n\u001b[1;32m     25\u001b[0m     [(\u001b[39m0.02\u001b[39m, \u001b[39m0.03\u001b[39m), (\u001b[39m0.04\u001b[39m, \u001b[39m0.07\u001b[39m), (\u001b[39m0.08\u001b[39m, \u001b[39m0.06\u001b[39m)],\n\u001b[1;32m     26\u001b[0m ]  \u001b[39m# Note these have been rescaled to be between [0, 1]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m scale \u001b[39m=\u001b[39m \u001b[39m1.1\u001b[39m\n\u001b[1;32m     30\u001b[0m train_transforms \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m     31\u001b[0m     [\n\u001b[1;32m     32\u001b[0m         A\u001b[39m.\u001b[39mLongestMaxSize(max_size\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(IMAGE_SIZE \u001b[39m*\u001b[39m scale)),\n\u001b[0;32m---> 33\u001b[0m         A\u001b[39m.\u001b[39mPadIfNeeded(\n\u001b[1;32m     34\u001b[0m             min_height\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(IMAGE_SIZE \u001b[39m*\u001b[39m scale),\n\u001b[1;32m     35\u001b[0m             min_width\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(IMAGE_SIZE \u001b[39m*\u001b[39m scale),\n\u001b[1;32m     36\u001b[0m             border_mode\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mBORDER_CONSTANT,\n\u001b[1;32m     37\u001b[0m         ),\n\u001b[1;32m     38\u001b[0m         A\u001b[39m.\u001b[39mRandomCrop(width\u001b[39m=\u001b[39mIMAGE_SIZE, height\u001b[39m=\u001b[39mIMAGE_SIZE),\n\u001b[1;32m     39\u001b[0m         A\u001b[39m.\u001b[39mColorJitter(brightness\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m, contrast\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m, saturation\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m, hue\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m),\n\u001b[1;32m     40\u001b[0m         A\u001b[39m.\u001b[39mOneOf(\n\u001b[1;32m     41\u001b[0m             [\n\u001b[1;32m     42\u001b[0m                 A\u001b[39m.\u001b[39mShiftScaleRotate(\n\u001b[1;32m     43\u001b[0m                     rotate_limit\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, border_mode\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mBORDER_CONSTANT\n\u001b[1;32m     44\u001b[0m                 ),\n\u001b[1;32m     45\u001b[0m                 A\u001b[39m.\u001b[39mIAAAffine(shear\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     46\u001b[0m             ],\n\u001b[1;32m     47\u001b[0m             p\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m,\n\u001b[1;32m     48\u001b[0m         ),\n\u001b[1;32m     49\u001b[0m         A\u001b[39m.\u001b[39mHorizontalFlip(p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m),\n\u001b[1;32m     50\u001b[0m         A\u001b[39m.\u001b[39mBlur(p\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m),\n\u001b[1;32m     51\u001b[0m         A\u001b[39m.\u001b[39mCLAHE(p\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m),\n\u001b[1;32m     52\u001b[0m         A\u001b[39m.\u001b[39mPosterize(p\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m),\n\u001b[1;32m     53\u001b[0m         A\u001b[39m.\u001b[39mToGray(p\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m),\n\u001b[1;32m     54\u001b[0m         A\u001b[39m.\u001b[39mChannelShuffle(p\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m),\n\u001b[1;32m     55\u001b[0m         A\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], max_pixel_value\u001b[39m=\u001b[39m\u001b[39m255\u001b[39m,),\n\u001b[1;32m     56\u001b[0m         ToTensorV2(),\n\u001b[1;32m     57\u001b[0m     ],\n\u001b[1;32m     58\u001b[0m     bbox_params\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mBboxParams(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myolo\u001b[39m\u001b[39m\"\u001b[39m, min_visibility\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m, label_fields\u001b[39m=\u001b[39m[],),\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     60\u001b[0m test_transforms \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m     61\u001b[0m     [\n\u001b[1;32m     62\u001b[0m         A\u001b[39m.\u001b[39mLongestMaxSize(max_size\u001b[39m=\u001b[39mIMAGE_SIZE),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     bbox_params\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mBboxParams(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myolo\u001b[39m\u001b[39m\"\u001b[39m, min_visibility\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m, label_fields\u001b[39m=\u001b[39m[]),\n\u001b[1;32m     70\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/albumentations/core/validation.py:35\u001b[0m, in \u001b[0;36mValidatedTransformMeta.__new__.<locals>.custom_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m         full_kwargs[parameter_name] \u001b[39m=\u001b[39m parameter\u001b[39m.\u001b[39mdefault\n\u001b[1;32m     34\u001b[0m \u001b[39m# No try-except block needed as we want the exception to propagate naturally\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m config \u001b[39m=\u001b[39m dct[\u001b[39m\"\u001b[39m\u001b[39mInitSchema\u001b[39m\u001b[39m\"\u001b[39m](\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfull_kwargs)\n\u001b[1;32m     37\u001b[0m validated_kwargs \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mmodel_dump()\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m name_arg \u001b[39min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/main.py:193\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    192\u001b[0m __tracebackhide__ \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__pydantic_validator__\u001b[39m.\u001b[39mvalidate_python(data, self_instance\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for InitSchema\n  Value error, If 'border_mode' is set to 'BORDER_CONSTANT', 'value' must be provided. [type=value_error, input_value={'min_height': 457, 'min_..._apply': None, 'p': 1.0}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error"
     ]
    }
   ],
   "source": [
    "DATASET = 'PASCAL_VOC'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# seed_everything()  # If you want deterministic behavior\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 416\n",
    "NUM_CLASSES = 20\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_EPOCHS = 100\n",
    "CONF_THRESHOLD = 0.05\n",
    "MAP_IOU_THRESH = 0.5\n",
    "NMS_IOU_THRESH = 0.45\n",
    "S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_FILE = \"checkpoint.pth.tar\"\n",
    "IMG_DIR = DATASET + \"/images/\"\n",
    "LABEL_DIR = DATASET + \"/labels/\"\n",
    "\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]  # Note these have been rescaled to be between [0, 1]\n",
    "\n",
    "\n",
    "scale = 1.1\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(IMAGE_SIZE * scale),\n",
    "            min_width=int(IMAGE_SIZE * scale),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                A.IAAAffine(shear=15, p=0.5, mode=\"constant\"),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
    ")\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
