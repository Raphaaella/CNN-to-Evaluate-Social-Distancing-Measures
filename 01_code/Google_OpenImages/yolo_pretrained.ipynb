{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# setup chunk\n",
    "\n",
    "## for deep learning architecture and evaluation\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## for plotting\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## for preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "## set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001b46b0b82ee29\n",
      "0002ae796e1f8eb5\n",
      "0002a1b8cc4b8f92\n",
      "0000a1b2fba255e9\n",
      "00006b13c052138f\n",
      "0000f8aef032941e\n",
      "00010d873e81c61e\n",
      "0001c626b9afb50c\n",
      "00013d077c604328\n",
      "0002a3c01c926a49\n",
      "0001fa6ab562fd2a\n",
      "0000bcb094764718\n",
      "000002b66c9c498e\n",
      "00010bf498b64bab\n",
      "00005e7429a94ad4\n",
      "0000fcb8ed0ea243\n",
      "00011aec5d7324f4\n",
      "0001b2b3b13cfbe4\n",
      "0002b83c86da3294\n",
      "0001e27f4b156f49\n",
      "00004b19ca2c952f\n",
      "0002a06d31985d69\n",
      "0000ce19115ae401\n",
      "00010d5d0cd3e273\n",
      "0000aa810854dc2e\n",
      "0002ab17a812c0d6\n",
      "0000c4f95a9d5a54\n",
      "0002c799b0cd7412\n",
      "0001e595b536c9ec\n",
      "00006bdb1eb5cd74\n",
      "00003e2837c7b728\n",
      "00001bcc92282a38\n",
      "00009cadede2ed69\n",
      "0000f8604a4e2cfe\n",
      "0000dde1f8ec7be1\n",
      "0001c43f78cd23e1\n",
      "0000c33c6f4b8518\n",
      "0000a4e648c5897f\n",
      "0001a1f45ad8e824\n",
      "0000f53faa4d14c3\n",
      "0000a90019e380dc\n",
      "0000b3e5921ab7ff\n",
      "00008d167563158c\n",
      "0001d48938a45d49\n",
      "0000d59fa570d973\n",
      "00010f041a2a6fa5\n",
      "0000bdfa52154160\n",
      "0000eb5027281f2a\n",
      "0000f509689e349c\n",
      "0000b4b26ef88376\n",
      "000101475b6bc944\n",
      "000033469fb48bc1\n",
      "00019bc020b24b32\n",
      "000045257f66b9e2\n",
      "0002347a67b7a730\n",
      "00026a2701e143f5\n",
      "0001386327595826\n",
      "0002337b77943386\n",
      "0001143bfa4f6ae7\n",
      "00023f2cf3334fdc\n",
      "000140f27cb00816\n",
      "000067f3419a12fe\n",
      "0000615b5a80f660\n",
      "000085351f54575c\n",
      "0000418f7ade0445\n",
      "000096726fd6c6c8\n",
      "0002912682d64112\n",
      "000072a05feb492a\n",
      "00027ae34cb09507\n",
      "0000432bf1241812\n",
      "000020780ccee28d\n",
      "00027d037fec5402\n",
      "00019ee392d03976\n",
      "000191087c2c9533\n",
      "000256419f7c57d8\n",
      "000266c1e4c17092\n",
      "00003223e04e2e66\n",
      "00019e09fe61747c\n",
      "000023aa04ab09ed\n",
      "0000071d71a0a6f6\n",
      "00017614428567ff\n",
      "0000880f55542351\n",
      "000132a014faadaf\n",
      "000213f8efef7523\n",
      "00028844acae6fae\n",
      "00026912315fd226\n",
      "000284a717e801dd\n",
      "0002024f996741eb\n",
      "00008044f1bbfde7\n",
      "000265efe8492bd9\n",
      "000243216b5a8e54\n",
      "00018571f3c267da\n",
      "00025431f1dca713\n",
      "0002172d94dfe46c\n",
      "000037c2dd414b46\n",
      "00027f4e7a1c370f\n",
      "000208a3f3477628\n",
      "000156c75d1ef7b2\n",
      "000134de87dd044b\n",
      "0000608cc97a2b17\n",
      "Annotation files have been created.\n"
     ]
    }
   ],
   "source": [
    "image_folder = '../../02_data/Google_OpenImages/images'\n",
    "label_folder = '../../02_data/Google_OpenImages/labels'\n",
    "annotation_file = '../../02_data/Google_OpenImages/filtered_person_annotations.csv'\n",
    "\n",
    "df = pd.read_csv(annotation_file)\n",
    "\n",
    "# Convert bounding boxes to YOLO format\n",
    "def convert_bbox_to_yolo_format(xmin, ymin, xmax, ymax):\n",
    "    center_x = (xmin + xmax) / 2\n",
    "    center_y = (ymin + ymax) / 2\n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "    return center_x, center_y, w, h\n",
    "\n",
    "# Process images in train and validation folders\n",
    "for split in ['train', 'validation']:\n",
    "    split_image_folder = os.path.join(image_folder, split)\n",
    "    split_label_folder = os.path.join(label_folder, split)\n",
    "    \n",
    "    # Process each image in the split folder\n",
    "    for image_file in os.listdir(split_image_folder):\n",
    "        if not image_file.lower().endswith(('.jpg', '.png')):  # Adjust if needed\n",
    "            continue\n",
    "\n",
    "        image_id = os.path.splitext(image_file)[0]\n",
    "        label_file = os.path.join(split_label_folder, image_id.replace('.jpg', '.txt'))\n",
    "\n",
    "        # Filter annotations for the current image\n",
    "        image_annotations = df[df['ImageID'] == image_id]\n",
    "\n",
    "        with open(label_file, 'w') as f:\n",
    "            for _, row in image_annotations.iterrows():\n",
    "                xmin, ymin, xmax, ymax = row[['XMin', 'YMin', 'XMax', 'YMax']]\n",
    "                class_id = 0  # Assuming single class (person)\n",
    "                center_x, center_y, w, h = convert_bbox_to_yolo_format(xmin, ymin, xmax, ymax)\n",
    "                f.write(f\"{class_id} {center_x} {center_y} {w} {h}\\n\")\n",
    "\n",
    "print(\"Annotation files have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov8n.yaml\")  # build a new model from YAML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file with box annotations\n",
    "person_annotations = pd.read_csv('../../02_data/Google_OpenImages/filtered_person_annotations.csv')\n",
    "\n",
    "# specify image folder\n",
    "image_folder = '../../02_data/Google_OpenImages/person_images_small/'\n",
    "\n",
    "# Count the number of bounding boxes per image\n",
    "image_counts = person_annotations.groupby('ImageID').size().reset_index(name='Count')\n",
    "\n",
    "# Convert to dictionary for easy lookup\n",
    "image_counts_dict = dict(zip(image_counts['ImageID'], image_counts['Count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(image_counts_dict.keys())\n",
    "train_keys = image_ids[:50]\n",
    "val_keys = image_ids[50:100]\n",
    "\n",
    "train_dict = {k: image_counts_dict[k] for k in train_keys}\n",
    "val_dict = {k: image_counts_dict[k] for k in val_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLODataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, annotation_file, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.annotation_file = annotation_file\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load image file paths and annotations\n",
    "        self.image_files = []\n",
    "        self.annotations = {}\n",
    "        with open(annotation_file, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip().split(',')\n",
    "                image_id = line[0]\n",
    "                bbox = [float(x) for x in line[1:]]\n",
    "                \n",
    "                if image_id not in self.annotations:\n",
    "                    self.annotations[image_id] = []\n",
    "                self.annotations[image_id].append(bbox)\n",
    "                \n",
    "        self.image_files = [os.path.join(image_folder, img) for img in self.annotations.keys()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_files[idx]\n",
    "        image_id = os.path.basename(image_path)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "        \n",
    "        # Load annotations\n",
    "        bboxes = self.annotations[image_id]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for bbox in bboxes:\n",
    "            # Convert bounding box to YOLO format\n",
    "            class_id = 0  # Assuming single class for people\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            center_x = (xmin + xmax) / 2 / width\n",
    "            center_y = (ymin + ymax) / 2 / height\n",
    "            w = (xmax - xmin) / width\n",
    "            h = (ymax - ymin) / height\n",
    "            boxes.append([class_id, center_x, center_y, w, h])\n",
    "            labels.append(class_id)\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transformation objects\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # rescale to uniform size (rescaling can lead to distortions if input not square)\n",
    "    transforms.RandomHorizontalFlip(), # randomly flip horizontally\n",
    "    transforms.RandomRotation(10), # randomly rotate by 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)), # random affine transformations\n",
    "    transforms.ToTensor(), # transform to tensor, brings pixels to range (0, 1)\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # normalize to range (-1, 1) for all three channels\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = YOLODataset(image_folder, train_dict, transform=train_transform)\n",
    "val_dataset = YOLODataset(image_folder, val_dict, transform=val_transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
