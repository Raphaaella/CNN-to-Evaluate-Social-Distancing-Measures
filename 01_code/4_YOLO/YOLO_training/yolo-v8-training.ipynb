{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T11:40:01.295553Z","iopub.status.busy":"2024-08-17T11:40:01.295299Z","iopub.status.idle":"2024-08-17T11:40:03.904803Z","shell.execute_reply":"2024-08-17T11:40:03.903775Z","shell.execute_reply.started":"2024-08-17T11:40:01.295529Z"},"trusted":true},"outputs":[{"data":{"text/plain":[]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.init(mode=\"disabled\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T11:40:03.906779Z","iopub.status.busy":"2024-08-17T11:40:03.906411Z","iopub.status.idle":"2024-08-17T11:40:20.026804Z","shell.execute_reply":"2024-08-17T11:40:20.025860Z","shell.execute_reply.started":"2024-08-17T11:40:03.906755Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.78-py3-none-any.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m615.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\n","Requirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\n","Requirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\n","Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\n","Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.0)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.5.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Downloading ultralytics-8.2.78-py3-none-any.whl (869 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m869.0/869.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n","Installing collected packages: ultralytics-thop, ultralytics\n","Successfully installed ultralytics-8.2.78 ultralytics-thop-2.0.0\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T11:40:20.028420Z","iopub.status.busy":"2024-08-17T11:40:20.028131Z","iopub.status.idle":"2024-08-17T11:40:26.461409Z","shell.execute_reply":"2024-08-17T11:40:26.460603Z","shell.execute_reply.started":"2024-08-17T11:40:20.028396Z"},"trusted":true},"outputs":[],"source":["from ultralytics import YOLO\n","import torch\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["### Train the Yolo model "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T11:40:26.464558Z","iopub.status.busy":"2024-08-17T11:40:26.463954Z","iopub.status.idle":"2024-08-17T12:26:58.887938Z","shell.execute_reply":"2024-08-17T12:26:58.887051Z","shell.execute_reply.started":"2024-08-17T11:40:26.464519Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.25M/6.25M [00:00<00:00, 71.7MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.2.78 üöÄ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/kaggle/input/yolo-train-oxford/YOLO/yolo.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 13.5MB/s]\n","2024-08-17 11:40:29,029\tINFO util.py:124 -- Outdated packages:\n","  ipywidgets==7.7.1 found, needs ipywidgets>=8\n","Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","2024-08-17 11:40:30,384\tINFO util.py:124 -- Outdated packages:\n","  ipywidgets==7.7.1 found, needs ipywidgets>=8\n","Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"]},{"name":"stdout","output_type":"stream","text":["Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n"," 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n"," 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n"," 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n","Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n","\n","Transferred 319/355 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/yolo-train-oxford/YOLO/Training... 2163 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2163/2163 [00:08<00:00, 251.41it/s]"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /kaggle/input/yolo-train-oxford/YOLO/Training/1736.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /kaggle/input/yolo-train-oxford/YOLO/Training/1737.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /kaggle/input/yolo-train-oxford/YOLO/Training/470.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /kaggle/input/yolo-train-oxford/YOLO/Training/733.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /kaggle/input/yolo-train-oxford/YOLO/Training/734.jpg: 1 duplicate labels removed\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è Cache directory /kaggle/input/yolo-train-oxford/YOLO is not writeable, cache not saved.\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/yolo-train-oxford/YOLO/Validation... 617 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 617/617 [00:02<00:00, 274.44it/s]"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /kaggle/input/yolo-train-oxford/YOLO/Validation/1738.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è Cache directory /kaggle/input/yolo-train-oxford/YOLO is not writeable, cache not saved.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Plotting labels to runs/detect/train/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ‚úÖ\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/detect/train\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      1/100      2.69G      2.231      1.942      1.423        118        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:31<00:00,  4.29it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.06it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.513      0.609      0.576      0.224\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      2/100       2.7G      1.844      1.452       1.25         66        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.08it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.683      0.701      0.759      0.326\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      3/100      2.89G      1.747      1.293      1.215         43        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.15it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.771      0.751      0.831      0.343\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      4/100      2.59G      1.714      1.188      1.193         84        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.14it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.76it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.829      0.768      0.859      0.437\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      5/100      2.92G      1.608      1.063      1.148         53        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.71it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.811      0.756      0.856      0.411\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      6/100      2.72G       1.57     0.9866      1.133         80        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.17it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.90it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.877      0.813      0.911      0.514\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      7/100      3.28G       1.52     0.9351      1.114         67        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.16it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.84it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.877      0.844      0.923      0.535\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      8/100      2.84G      1.495     0.8955      1.096         95        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.73it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.902      0.868       0.94      0.541\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["      9/100      2.62G      1.457     0.8658      1.081         58        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.61it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.911      0.874      0.947      0.532\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     10/100      3.04G      1.443     0.8425      1.081         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.17it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.925      0.858      0.944      0.532\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     11/100      2.92G      1.429      0.818      1.072         55        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.74it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.934      0.883      0.958      0.571\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     12/100      2.77G      1.418     0.7961      1.055         47        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.16it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.81it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.922      0.895      0.958      0.583\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     13/100      2.58G      1.383     0.7728       1.05         52        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.84it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.943      0.897      0.965      0.602\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     14/100      3.13G       1.37     0.7559      1.038         49        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.17it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.80it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.925      0.894      0.954      0.594\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     15/100      2.84G      1.371     0.7517      1.046         46        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.70it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.942      0.916       0.96      0.598\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     16/100      2.84G      1.345     0.7313      1.033         68        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.24it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.80it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.947      0.912       0.97      0.609\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     17/100      2.94G      1.328     0.7212      1.023         88        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.98it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.951      0.909      0.972      0.617\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     18/100      2.78G       1.32     0.7123      1.024        116        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.85it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.954      0.911      0.972      0.625\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     19/100      2.58G      1.315     0.6985      1.026         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.81it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.954      0.922      0.975      0.624\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     20/100      2.76G      1.296     0.6861      1.013         69        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.966      0.923      0.979      0.629\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     21/100      2.93G       1.29     0.6737      1.006         92        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.77it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.954      0.926      0.976      0.633\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     22/100      2.97G      1.294     0.6778      1.006         72        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.14it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.81it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.963      0.927      0.979      0.642\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     23/100      3.07G      1.285     0.6665       1.01         80        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.73it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.958       0.93      0.976      0.637\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     24/100      3.13G      1.271     0.6566          1         68        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.89it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.965      0.936      0.981      0.649\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     25/100       3.1G      1.262     0.6534     0.9973         28        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.13it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.88it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.963      0.934       0.98      0.651\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     26/100      2.82G      1.268     0.6614      1.002         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.22it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.97it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.962      0.929       0.98      0.638\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     27/100       2.8G       1.26     0.6445      1.003         30        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.87it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.964      0.935      0.982      0.653\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     28/100      2.68G       1.25     0.6393      0.997         68        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.86it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.964      0.938      0.981      0.654\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     29/100      2.72G       1.25     0.6383     0.9925         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.87it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.968      0.933      0.983       0.66\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     30/100         3G      1.231     0.6204     0.9882         68        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.22it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.79it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495       0.97      0.937      0.982      0.648\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     31/100      2.58G      1.232      0.626     0.9931         37        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.26it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.79it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.965      0.945      0.985      0.664\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     32/100      2.51G      1.234     0.6156     0.9874         54        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.12it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.89it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.968       0.94      0.984      0.659\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     33/100      2.82G      1.217     0.6084     0.9801        110        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.963      0.949      0.985      0.666\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     34/100      2.94G      1.218     0.6076     0.9823         65        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.16it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.966      0.948      0.985      0.673\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     35/100      2.65G      1.214     0.6049     0.9823         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.969      0.938      0.983      0.667\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     36/100      3.03G      1.214     0.6051     0.9828         52        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.22it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.89it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.974       0.95      0.987      0.661\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     37/100      3.18G      1.207     0.6009     0.9806         64        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.22it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.962      0.939      0.982      0.652\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     38/100      2.66G        1.2     0.5995     0.9781         65        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.24it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.974      0.946      0.986      0.663\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     39/100      3.13G      1.198      0.592     0.9767         63        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.84it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.968       0.95      0.986      0.671\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     40/100      2.69G      1.188     0.5855     0.9713         41        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.01it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.973      0.952      0.988      0.674\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     41/100      2.66G      1.199     0.5872     0.9759        106        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.99it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.974      0.954      0.988      0.685\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     42/100       3.1G      1.179     0.5793      0.967         77        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.92it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495       0.97      0.955      0.988      0.683\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     43/100      2.47G      1.176     0.5818     0.9669         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.80it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.978      0.947      0.987      0.679\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     44/100      2.72G      1.183     0.5778     0.9695         96        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.24it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.88it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.976      0.955      0.988       0.68\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     45/100      2.69G      1.174     0.5724     0.9646         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.23it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.88it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.972      0.956      0.988      0.674\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     46/100       2.9G      1.175     0.5743     0.9685         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.22it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.971      0.959      0.989      0.679\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     47/100      3.26G      1.169     0.5722     0.9643         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.17it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.91it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.976      0.961      0.989      0.689\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     48/100      3.29G      1.167     0.5652     0.9587         48        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.15it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.96it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.976      0.959      0.989      0.691\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     49/100      2.92G      1.161     0.5631     0.9601        117        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.84it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.974       0.96      0.989      0.686\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     50/100      3.18G      1.162     0.5637     0.9608         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.22it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.76it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.971      0.962      0.989      0.687\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     51/100      3.07G      1.162     0.5638     0.9639         54        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.91it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.979       0.96       0.99      0.687\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     52/100         3G      1.153     0.5578     0.9615        123        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.92it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.975      0.959      0.989      0.687\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     53/100      2.95G      1.153     0.5559     0.9601         28        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.89it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.974      0.959      0.989      0.688\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     54/100      2.86G      1.143     0.5509     0.9542         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.84it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.976      0.964       0.99      0.696\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     55/100      2.52G      1.143     0.5511     0.9532         53        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.85it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.978      0.956       0.99      0.695\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     56/100      3.11G      1.141     0.5518     0.9566         49        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.82it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495       0.98       0.96       0.99      0.694\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     57/100      2.87G      1.132     0.5386     0.9534         88        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.22it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.73it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.978      0.963       0.99      0.701\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     58/100      2.54G      1.137     0.5418     0.9528         62        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.23it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.81it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.977      0.965       0.99      0.692\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     59/100      2.96G       1.13     0.5423     0.9509         61        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.99it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.978      0.965      0.991      0.692\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     60/100      2.78G      1.131     0.5401      0.952         55        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.88it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.981      0.965      0.991      0.697\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     61/100      3.25G      1.125     0.5363     0.9504         51        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.17it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.79it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.979      0.967      0.991      0.699\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     62/100      2.71G      1.122     0.5338     0.9511         86        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.86it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.982      0.959       0.99        0.7\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     63/100      2.91G      1.125     0.5367     0.9502         54        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.04it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.981      0.965      0.991      0.702\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     64/100      2.76G      1.115     0.5268     0.9432         75        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.81it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.978      0.968      0.992      0.703\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     65/100      3.12G      1.126       0.53     0.9486         53        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.983      0.963      0.991      0.701\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     66/100      2.64G      1.113     0.5271     0.9517         36        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.15it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.79it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.982      0.968      0.992      0.707\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     67/100      2.92G      1.115     0.5257     0.9444         82        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.16it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.96it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984      0.964      0.992      0.705\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     68/100      3.21G      1.111      0.528     0.9417         76        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.92it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.978       0.97      0.991      0.708\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     69/100      2.73G      1.112     0.5242     0.9455         62        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.78it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.983      0.966      0.991      0.709\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     70/100      2.73G      1.111     0.5239     0.9418        117        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.92it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984      0.968      0.992      0.709\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     71/100       3.1G      1.099     0.5159     0.9392        134        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.22it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.80it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495       0.98       0.97      0.991      0.706\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     72/100      2.94G      1.092     0.5138     0.9395         91        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.21it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.82it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984      0.967      0.992      0.705\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     73/100      2.96G      1.099     0.5158     0.9402         71        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.88it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984      0.969      0.992       0.71\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     74/100      2.88G      1.093     0.5139     0.9407         79        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.16it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.79it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.982       0.97      0.992      0.707\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     75/100      3.12G      1.091     0.5148     0.9389         52        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.73it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984       0.97      0.992      0.712\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     76/100      3.08G      1.092     0.5106     0.9397         37        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.16it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.86it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.982      0.971      0.992       0.71\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     77/100      2.97G      1.091     0.5085     0.9397         50        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.19it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.95it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984      0.969      0.992      0.712\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     78/100      2.88G      1.081     0.5061      0.933         57        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.74it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984       0.97      0.992      0.716\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     79/100      2.81G      1.076     0.5016     0.9342         65        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.85it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.983       0.97      0.992      0.715\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     80/100      3.08G      1.081     0.5039     0.9331        117        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.90it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984      0.966      0.992      0.715\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     81/100      2.41G      1.073     0.5013     0.9315         73        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.16it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.97it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.983      0.972      0.993      0.714\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     82/100      3.19G       1.07     0.4974     0.9335         67        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.94it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.987       0.97      0.992      0.715\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     83/100      2.84G       1.07     0.4973     0.9334         67        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.13it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.77it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.987       0.97      0.993      0.719\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     84/100      2.81G      1.067     0.4953     0.9297         80        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.15it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.82it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984      0.973      0.993      0.718\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     85/100      2.67G      1.068     0.4951     0.9305        111        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.18it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.62it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.985      0.972      0.993      0.716\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     86/100      2.71G       1.06     0.4926     0.9282         38        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.987      0.971      0.993       0.72\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     87/100      2.93G      1.063     0.4921     0.9297         98        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:21<00:00,  6.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.90it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.985      0.974      0.993      0.718\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     88/100       2.8G       1.06     0.4898     0.9321         32        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.14it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.97it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.983      0.974      0.992      0.718\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     89/100      2.91G      1.055     0.4892     0.9266         34        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.16it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.80it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.987      0.973      0.993       0.72\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     90/100       2.7G      1.059      0.492     0.9288        112        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:22<00:00,  6.15it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.80it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.984      0.973      0.993      0.722\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     91/100      2.38G      1.063     0.4626     0.9477         32        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:25<00:00,  5.41it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.985      0.974      0.993       0.72\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     92/100      2.21G      1.034     0.4487     0.9373         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.53it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.83it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.985      0.976      0.993      0.724\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     93/100      2.17G       1.03     0.4447     0.9366         45        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.53it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.70it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.986      0.976      0.993      0.726\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     94/100       2.2G      1.027     0.4454     0.9379         40        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.50it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.78it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.987      0.976      0.993      0.726\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     95/100      2.18G      1.027     0.4431     0.9334         40        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.51it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.81it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.985      0.974      0.993      0.728\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     96/100      2.21G      1.024     0.4421     0.9342         42        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.52it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.75it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.987      0.975      0.993      0.728\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     97/100       2.2G      1.017     0.4385     0.9313         43        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.50it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.97it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.985      0.977      0.993      0.726\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     98/100      2.18G      1.017     0.4381     0.9314         39        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.53it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.73it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.985      0.978      0.993      0.726\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["     99/100       2.2G      1.014     0.4371     0.9312         54        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.52it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.91it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.985      0.977      0.993      0.727\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["    100/100      2.19G       1.01     0.4355     0.9294         47        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 136/136 [00:20<00:00,  6.48it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:04<00:00,  4.97it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.986      0.976      0.993      0.729\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","100 epochs completed in 0.755 hours.\n","Optimizer stripped from runs/detect/train/weights/last.pt, 6.3MB\n","Optimizer stripped from runs/detect/train/weights/best.pt, 6.3MB\n","\n","Validating runs/detect/train/weights/best.pt...\n","Ultralytics YOLOv8.2.78 üöÄ Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15095MiB)\n","Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"]},{"name":"stderr","output_type":"stream","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:15<00:00,  1.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all        617       9495      0.986      0.976      0.993      0.729\n","Speed: 0.2ms preprocess, 2.1ms inference, 0.0ms loss, 1.7ms postprocess per image\n","Results saved to \u001b[1mruns/detect/train\u001b[0m\n"]}],"source":["# Training using the pretrained nano version of the model \n","yolo = YOLO(\"yolov8n.pt\")\n","results = yolo.train(data=\"/kaggle/input/yolo-train-oxford/YOLO/yolo.yaml\", epochs=100) # training will only work in kaggle with specified dataset, because of the yaml config file "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T12:50:33.203642Z","iopub.status.busy":"2024-08-17T12:50:33.202712Z","iopub.status.idle":"2024-08-17T12:50:33.267401Z","shell.execute_reply":"2024-08-17T12:50:33.266524Z","shell.execute_reply.started":"2024-08-17T12:50:33.203601Z"},"trusted":true},"outputs":[],"source":["# load the model after traing - path \n","yolo_trained = YOLO('DLSS_project/CNN-to-Evaluate-Social-Distancing-Measures/01_code/4_YOLO/YOLO_trained_100_oxford') # path might need adjustion\n","#yolo_trained = YOLO(\"yolov8n.pt\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T12:50:33.830533Z","iopub.status.busy":"2024-08-17T12:50:33.830190Z","iopub.status.idle":"2024-08-17T12:50:33.836098Z","shell.execute_reply":"2024-08-17T12:50:33.835043Z","shell.execute_reply.started":"2024-08-17T12:50:33.830500Z"},"trusted":true},"outputs":[],"source":["# function to count ground truth labels in annotations\n","def count_people_from_labels(label_path):\n","    person_count = 0\n","    \n","    # Open the label file and count entries where class_id is 0 (person)\n","    with open(label_path, 'r') as file:\n","        for line in file:\n","            class_id = int(line.split()[0])\n","            if class_id == 0:  # 0 is the class ID for 'person'\n","                person_count += 1\n","    \n","    return person_count\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T12:50:34.509841Z","iopub.status.busy":"2024-08-17T12:50:34.509514Z","iopub.status.idle":"2024-08-17T12:50:34.516602Z","shell.execute_reply":"2024-08-17T12:50:34.515710Z","shell.execute_reply.started":"2024-08-17T12:50:34.509813Z"},"trusted":true},"outputs":[],"source":["# function to count preditions of persons from YOLO \n","from PIL import Image\n","import os\n","import numpy as np\n","def count_people_from_yolo(image_path):\n","    # Load the image\n","    img = Image.open(image_path)\n","    \n","    # Get the detections directly\n","    results = yolo_trained.predict(img)\n","    boxes = results[0].boxes.xyxy.tolist()\n","    classes = results[0].boxes.cls.tolist()\n","    detections = np.hstack((boxes,np.array(classes).reshape(-1, 1)))\n","    \n","    # Count the number of people detected\n","    person_count = 0\n","    for det in detections:\n","        class_id = int(det[-1])\n","        if class_id == 0:  # YOLO class ID for 'person' is 0\n","            person_count += 1\n","    \n","    return person_count"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T12:50:35.444442Z","iopub.status.busy":"2024-08-17T12:50:35.443859Z","iopub.status.idle":"2024-08-17T12:50:42.109759Z","shell.execute_reply":"2024-08-17T12:50:42.108920Z","shell.execute_reply.started":"2024-08-17T12:50:35.444413Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","0: 640x640 23 persons, 8.4ms\n","Speed: 3.0ms preprocess, 8.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","623.jpg: Actual = 20, Predicted = 23\n","\n","0: 640x640 19 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","771.jpg: Actual = 17, Predicted = 19\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2029.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 26 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1385.jpg: Actual = 25, Predicted = 26\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","333.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 20 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","537.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","654.jpg: Actual = 20, Predicted = 21\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1339.jpg: Actual = 21, Predicted = 21\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2057.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2137.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","6.jpg: Actual = 13, Predicted = 15\n","\n","0: 640x640 19 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","708.jpg: Actual = 19, Predicted = 19\n","\n","0: 640x640 22 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1840.jpg: Actual = 19, Predicted = 22\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2580.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 20 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","501.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2279.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 18 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","576.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 28 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1451.jpg: Actual = 27, Predicted = 28\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","189.jpg: Actual = 8, Predicted = 9\n","\n","0: 640x640 15 persons, 7.4ms\n","Speed: 1.9ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2403.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2626.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 19 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1815.jpg: Actual = 20, Predicted = 19\n","\n","0: 640x640 10 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","131.jpg: Actual = 10, Predicted = 10\n","\n","0: 640x640 10 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1058.jpg: Actual = 9, Predicted = 10\n","\n","0: 640x640 13 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","892.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2476.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 13 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2795.jpg: Actual = 12, Predicted = 13\n","\n","0: 640x640 12 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","283.jpg: Actual = 12, Predicted = 12\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","359.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 19 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1709.jpg: Actual = 18, Predicted = 19\n","\n","0: 640x640 12 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","289.jpg: Actual = 12, Predicted = 12\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2842.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 7 persons, 7.3ms\n","Speed: 2.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2890.jpg: Actual = 6, Predicted = 7\n","\n","0: 640x640 12 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","295.jpg: Actual = 12, Predicted = 12\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2592.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 2.0ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","829.jpg: Actual = 15, Predicted = 16\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 2.4ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2694.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 12 persons, 7.6ms\n","Speed: 1.8ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","291.jpg: Actual = 12, Predicted = 12\n","\n","0: 640x640 20 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","1778.jpg: Actual = 19, Predicted = 20\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","2503.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2840.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","665.jpg: Actual = 19, Predicted = 21\n","\n","0: 640x640 18 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1183.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 6 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2900.jpg: Actual = 6, Predicted = 6\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2661.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 18 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","589.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1108.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 12 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","35.jpg: Actual = 12, Predicted = 12\n","\n","0: 640x640 7 persons, 7.3ms\n","Speed: 2.1ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2944.jpg: Actual = 7, Predicted = 7\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2109.jpg: Actual = 13, Predicted = 15\n","\n","0: 640x640 7 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2951.jpg: Actual = 7, Predicted = 7\n","\n","0: 640x640 23 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1366.jpg: Actual = 23, Predicted = 23\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2671.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 20 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","601.jpg: Actual = 18, Predicted = 20\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2850.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1997.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 13 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2541.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 8 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","230.jpg: Actual = 9, Predicted = 8\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","872.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 18 persons, 7.5ms\n","Speed: 1.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2352.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3047.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1919.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 20 persons, 8.5ms\n","Speed: 1.8ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","538.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 13 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","57.jpg: Actual = 9, Predicted = 13\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","227.jpg: Actual = 10, Predicted = 9\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1095.jpg: Actual = 14, Predicted = 16\n","\n","0: 640x640 8 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","3054.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","374.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1962.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2614.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 8 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3006.jpg: Actual = 7, Predicted = 8\n","\n","0: 640x640 20 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","701.jpg: Actual = 19, Predicted = 20\n","\n","0: 640x640 22 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1767.jpg: Actual = 19, Predicted = 22\n","\n","0: 640x640 28 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1544.jpg: Actual = 28, Predicted = 28\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1324.jpg: Actual = 21, Predicted = 21\n","\n","0: 640x640 27 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1523.jpg: Actual = 24, Predicted = 27\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1259.jpg: Actual = 21, Predicted = 21\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2310.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","2296.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","186.jpg: Actual = 8, Predicted = 9\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","69.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 17 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","414.jpg: Actual = 17, Predicted = 17\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1980.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2822.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 28 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","1503.jpg: Actual = 25, Predicted = 28\n","\n","0: 640x640 23 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1287.jpg: Actual = 21, Predicted = 23\n","\n","0: 640x640 12 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","943.jpg: Actual = 10, Predicted = 12\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","535.jpg: Actual = 20, Predicted = 21\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","393.jpg: Actual = 14, Predicted = 16\n","\n","0: 640x640 9 persons, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1053.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 27 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1462.jpg: Actual = 27, Predicted = 27\n","\n","0: 640x640 11 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","276.jpg: Actual = 11, Predicted = 11\n","\n","0: 640x640 16 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2382.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 13 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","903.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 21 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","716.jpg: Actual = 18, Predicted = 21\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1901.jpg: Actual = 16, Predicted = 15\n","\n","0: 640x640 13 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2534.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 18 persons, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","574.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 11 persons, 7.2ms\n","Speed: 2.2ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","44.jpg: Actual = 11, Predicted = 11\n","\n","0: 640x640 22 persons, 7.1ms\n","Speed: 2.0ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1839.jpg: Actual = 19, Predicted = 22\n","\n","0: 640x640 18 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","561.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 26 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1550.jpg: Actual = 28, Predicted = 26\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2140.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 15 persons, 7.2ms\n","Speed: 2.3ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2450.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2594.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 9 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2829.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 8 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","179.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 21 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","1261.jpg: Actual = 22, Predicted = 21\n","\n","0: 640x640 17 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","419.jpg: Actual = 17, Predicted = 17\n","\n","0: 640x640 8 persons, 7.1ms\n","Speed: 2.1ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1038.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 22 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1353.jpg: Actual = 22, Predicted = 22\n","\n","0: 640x640 22 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1290.jpg: Actual = 21, Predicted = 22\n","\n","0: 640x640 14 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","317.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 16 persons, 7.1ms\n","Speed: 2.4ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2101.jpg: Actual = 14, Predicted = 16\n","\n","0: 640x640 19 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1816.jpg: Actual = 20, Predicted = 19\n","\n","0: 640x640 10 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","145.jpg: Actual = 9, Predicted = 10\n","\n","0: 640x640 16 persons, 7.1ms\n","Speed: 1.9ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2064.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 13 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","23.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2401.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 14 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2787.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2736.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 16 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2102.jpg: Actual = 14, Predicted = 16\n","\n","0: 640x640 7 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","166.jpg: Actual = 7, Predicted = 7\n","\n","0: 640x640 23 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1730.jpg: Actual = 21, Predicted = 23\n","\n","0: 640x640 20 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","430.jpg: Actual = 18, Predicted = 20\n","\n","0: 640x640 28 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","1450.jpg: Actual = 27, Predicted = 28\n","\n","0: 640x640 17 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2344.jpg: Actual = 17, Predicted = 17\n","\n","0: 640x640 21 persons, 7.1ms\n","Speed: 1.9ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1754.jpg: Actual = 19, Predicted = 21\n","\n","0: 640x640 20 persons, 7.1ms\n","Speed: 2.1ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1244.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 19 persons, 7.1ms\n","Speed: 2.0ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1684.jpg: Actual = 18, Predicted = 19\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2613.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 16 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1140.jpg: Actual = 17, Predicted = 16\n","\n","0: 640x640 7 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","2885.jpg: Actual = 7, Predicted = 7\n","\n","0: 640x640 18 persons, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","809.jpg: Actual = 17, Predicted = 18\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 2.1ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2723.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 14 persons, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2118.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 8 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","174.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2136.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 6 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2889.jpg: Actual = 6, Predicted = 6\n","\n","0: 640x640 18 persons, 7.1ms\n","Speed: 2.1ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","854.jpg: Actual = 14, Predicted = 18\n","\n","0: 640x640 18 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2098.jpg: Actual = 14, Predicted = 18\n","\n","0: 640x640 6 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","3018.jpg: Actual = 6, Predicted = 6\n","\n","0: 640x640 14 persons, 7.1ms\n","Speed: 1.9ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","332.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 26 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1529.jpg: Actual = 26, Predicted = 26\n","\n","0: 640x640 19 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","784.jpg: Actual = 18, Predicted = 19\n","\n","0: 640x640 21 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","633.jpg: Actual = 20, Predicted = 21\n","\n","0: 640x640 15 persons, 7.2ms\n","Speed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2240.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 7 persons, 7.1ms\n","Speed: 2.4ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2939.jpg: Actual = 7, Predicted = 7\n","\n","0: 640x640 19 persons, 7.1ms\n","Speed: 1.9ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1627.jpg: Actual = 20, Predicted = 19\n","\n","0: 640x640 12 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","48.jpg: Actual = 11, Predicted = 12\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2760.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 14 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","918.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 15 persons, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2187.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 18 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","572.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 7 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","169.jpg: Actual = 7, Predicted = 7\n","\n","0: 640x640 21 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1743.jpg: Actual = 21, Predicted = 21\n","\n","0: 640x640 19 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","821.jpg: Actual = 15, Predicted = 19\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.9ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2744.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 17 persons, 7.1ms\n","Speed: 2.1ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2374.jpg: Actual = 16, Predicted = 17\n","\n","0: 640x640 9 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3080.jpg: Actual = 8, Predicted = 9\n","\n","0: 640x640 9 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3035.jpg: Actual = 8, Predicted = 9\n","\n","0: 640x640 10 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","88.jpg: Actual = 10, Predicted = 10\n","\n","0: 640x640 28 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1469.jpg: Actual = 25, Predicted = 28\n","\n","0: 640x640 22 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1345.jpg: Actual = 22, Predicted = 22\n","\n","0: 640x640 21 persons, 7.0ms\n","Speed: 2.0ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1797.jpg: Actual = 20, Predicted = 21\n","\n","0: 640x640 16 persons, 7.0ms\n","Speed: 2.1ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2380.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 9 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","148.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 7 persons, 7.0ms\n","Speed: 2.1ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2949.jpg: Actual = 7, Predicted = 7\n","\n","0: 640x640 18 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","693.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 18 persons, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1879.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 21 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1722.jpg: Actual = 19, Predicted = 21\n","\n","0: 640x640 19 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","650.jpg: Actual = 20, Predicted = 19\n","\n","0: 640x640 16 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2324.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 14 persons, 7.2ms\n","Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1917.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2024.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 23 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1368.jpg: Actual = 22, Predicted = 23\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 2.3ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2407.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 15 persons, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2663.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 20 persons, 7.2ms\n","Speed: 1.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","667.jpg: Actual = 19, Predicted = 20\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1107.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 8 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1035.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 11 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","940.jpg: Actual = 11, Predicted = 11\n","\n","0: 640x640 23 persons, 7.8ms\n","Speed: 1.8ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1362.jpg: Actual = 23, Predicted = 23\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2138.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 21 persons, 7.1ms\n","Speed: 2.3ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1765.jpg: Actual = 19, Predicted = 21\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2058.jpg: Actual = 16, Predicted = 15\n","\n","0: 640x640 26 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1397.jpg: Actual = 26, Predicted = 26\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2578.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 13 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2509.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 10 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1068.jpg: Actual = 10, Predicted = 10\n","\n","0: 640x640 8 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2990.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 13 persons, 7.0ms\n","Speed: 1.9ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","911.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 10 persons, 7.4ms\n","Speed: 1.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","957.jpg: Actual = 9, Predicted = 10\n","\n","0: 640x640 21 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","802.jpg: Actual = 18, Predicted = 21\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","321.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","1121.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 10 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","956.jpg: Actual = 9, Predicted = 10\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2095.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2754.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 6 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2912.jpg: Actual = 6, Predicted = 6\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","889.jpg: Actual = 12, Predicted = 14\n","\n","0: 640x640 28 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1454.jpg: Actual = 27, Predicted = 28\n","\n","0: 640x640 8 persons, 7.0ms\n","Speed: 2.1ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","183.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 25 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1588.jpg: Actual = 23, Predicted = 25\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2466.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 19 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","459.jpg: Actual = 20, Predicted = 19\n","\n","0: 640x640 6 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2908.jpg: Actual = 6, Predicted = 6\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 1.9ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2726.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 16 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1098.jpg: Actual = 14, Predicted = 16\n","\n","0: 640x640 10 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1000.jpg: Actual = 9, Predicted = 10\n","\n","0: 640x640 24 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1585.jpg: Actual = 24, Predicted = 24\n","\n","0: 640x640 14 persons, 8.1ms\n","Speed: 2.6ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2516.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 19 persons, 7.0ms\n","Speed: 2.3ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","557.jpg: Actual = 18, Predicted = 19\n","\n","0: 640x640 20 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","1237.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 12 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1950.jpg: Actual = 14, Predicted = 12\n","\n","0: 640x640 25 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1481.jpg: Actual = 25, Predicted = 25\n","\n","0: 640x640 16 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2291.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 19 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1843.jpg: Actual = 18, Predicted = 19\n","\n","0: 640x640 16 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","840.jpg: Actual = 15, Predicted = 16\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2634.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 13 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","11.jpg: Actual = 14, Predicted = 13\n","\n","0: 640x640 18 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2357.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 22 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","652.jpg: Actual = 20, Predicted = 22\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 2.1ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2759.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2126.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 9 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3045.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 20 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","472.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 12 persons, 7.0ms\n","Speed: 1.9ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","34.jpg: Actual = 12, Predicted = 12\n","\n","0: 640x640 27 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1415.jpg: Actual = 27, Predicted = 27\n","\n","0: 640x640 20 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","432.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 11 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1070.jpg: Actual = 10, Predicted = 11\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2013.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 9 persons, 7.0ms\n","Speed: 1.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3033.jpg: Actual = 8, Predicted = 9\n","\n","0: 640x640 22 persons, 7.0ms\n","Speed: 2.3ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","1305.jpg: Actual = 22, Predicted = 22\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","21.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 25 persons, 8.4ms\n","Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","1597.jpg: Actual = 24, Predicted = 25\n","\n","0: 640x640 15 persons, 7.0ms\n","Speed: 2.2ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2409.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 16 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2422.jpg: Actual = 15, Predicted = 16\n","\n","0: 640x640 19 persons, 7.0ms\n","Speed: 1.9ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1847.jpg: Actual = 18, Predicted = 19\n","\n","0: 640x640 17 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2430.jpg: Actual = 16, Predicted = 17\n","\n","0: 640x640 17 persons, 7.9ms\n","Speed: 2.6ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","855.jpg: Actual = 14, Predicted = 17\n","\n","0: 640x640 18 persons, 7.1ms\n","Speed: 1.9ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2350.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 13 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2546.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 14 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2502.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 8 persons, 7.0ms\n","Speed: 2.1ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","3032.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 21 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","657.jpg: Actual = 19, Predicted = 21\n","\n","0: 640x640 7 persons, 7.0ms\n","Speed: 1.8ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2943.jpg: Actual = 7, Predicted = 7\n","\n","0: 640x640 16 persons, 7.0ms\n","Speed: 1.9ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","348.jpg: Actual = 16, Predicted = 16\n","\n","0: 640x640 14 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1941.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 15 persons, 7.1ms\n","Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2402.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 13 persons, 7.1ms\n","Speed: 2.2ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2032.jpg: Actual = 13, Predicted = 13\n","\n","0: 640x640 14 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","923.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 14 persons, 7.1ms\n","Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1952.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 17 persons, 10.8ms\n","Speed: 2.2ms preprocess, 10.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","413.jpg: Actual = 17, Predicted = 17\n","\n","0: 640x640 22 persons, 7.5ms\n","Speed: 2.8ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","1275.jpg: Actual = 22, Predicted = 22\n","\n","0: 640x640 9 persons, 9.9ms\n","Speed: 2.7ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","2819.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 20 persons, 11.0ms\n","Speed: 2.2ms preprocess, 11.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1830.jpg: Actual = 19, Predicted = 20\n","\n","0: 640x640 18 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1699.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 23 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1723.jpg: Actual = 20, Predicted = 23\n","\n","0: 640x640 21 persons, 8.2ms\n","Speed: 1.8ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","522.jpg: Actual = 21, Predicted = 21\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","325.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 6 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2906.jpg: Actual = 6, Predicted = 6\n","\n","0: 640x640 29 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1438.jpg: Actual = 27, Predicted = 29\n","\n","0: 640x640 24 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","1623.jpg: Actual = 21, Predicted = 24\n","\n","0: 640x640 20 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","471.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 22 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1306.jpg: Actual = 22, Predicted = 22\n","\n","0: 640x640 17 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2371.jpg: Actual = 16, Predicted = 17\n","\n","0: 640x640 23 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","1365.jpg: Actual = 23, Predicted = 23\n","\n","0: 640x640 26 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1403.jpg: Actual = 26, Predicted = 26\n","\n","0: 640x640 7 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2930.jpg: Actual = 6, Predicted = 7\n","\n","0: 640x640 12 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","47.jpg: Actual = 12, Predicted = 12\n","\n","0: 640x640 8 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3048.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 6 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","3020.jpg: Actual = 6, Predicted = 6\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","857.jpg: Actual = 13, Predicted = 16\n","\n","0: 640x640 27 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1444.jpg: Actual = 26, Predicted = 27\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","324.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 20 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","1246.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2617.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","2007.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 18 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2359.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2124.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 20 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1226.jpg: Actual = 20, Predicted = 20\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1811.jpg: Actual = 20, Predicted = 21\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2215.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 27 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1516.jpg: Actual = 25, Predicted = 27\n","\n","0: 640x640 19 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1633.jpg: Actual = 19, Predicted = 19\n","\n","0: 640x640 23 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1367.jpg: Actual = 23, Predicted = 23\n","\n","0: 640x640 8 persons, 7.3ms\n","Speed: 2.0ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3069.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 22 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1732.jpg: Actual = 21, Predicted = 22\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","899.jpg: Actual = 13, Predicted = 14\n","\n","0: 640x640 26 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1587.jpg: Actual = 24, Predicted = 26\n","\n","0: 640x640 10 persons, 7.3ms\n","Speed: 2.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","969.jpg: Actual = 10, Predicted = 10\n","\n","0: 640x640 17 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1147.jpg: Actual = 17, Predicted = 17\n","\n","0: 640x640 26 persons, 7.3ms\n","Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1578.jpg: Actual = 24, Predicted = 26\n","\n","0: 640x640 22 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1321.jpg: Actual = 21, Predicted = 22\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2234.jpg: Actual = 15, Predicted = 15\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","881.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 9 persons, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","986.jpg: Actual = 9, Predicted = 9\n","\n","0: 640x640 18 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","593.jpg: Actual = 18, Predicted = 18\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2586.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","551.jpg: Actual = 19, Predicted = 21\n","\n","0: 640x640 21 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","1323.jpg: Actual = 21, Predicted = 21\n","\n","0: 640x640 15 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2198.jpg: Actual = 14, Predicted = 15\n","\n","0: 640x640 8 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","3070.jpg: Actual = 8, Predicted = 8\n","\n","0: 640x640 19 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","461.jpg: Actual = 20, Predicted = 19\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2212.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 14 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n","2461.jpg: Actual = 14, Predicted = 14\n","\n","0: 640x640 10 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","2861.jpg: Actual = 10, Predicted = 10\n","\n","0: 640x640 10 persons, 7.3ms\n","Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n","90.jpg: Actual = 10, Predicted = 10\n","\n","0: 640x640 16 persons, 7.3ms\n","Speed: 1.9ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","1979.jpg: Actual = 15, Predicted = 16\n"]}],"source":["# Count both ground truth and predictions for the Testing set of Oxford \n","folder = '/kaggle/input/yolo-train-oxford/YOLO/Testing'\n","\n","actual_counts = []\n","predicted_counts = []\n","\n","for filename in os.listdir(folder):\n","    # Process only .jpg files\n","    if filename.endswith('.jpg'):\n","        # Get corresponding image and label paths\n","        image_path = os.path.join(folder, filename)\n","        label_path = os.path.join(folder, os.path.splitext(filename)[0] + '.txt')\n","        \n","        # Ensure the label file exists\n","        if os.path.exists(label_path):\n","            # Count people based on labels (ground truth)\n","            actual_count = count_people_from_labels(label_path)\n","            \n","            # Count people using YOLO model\n","            predicted_count = count_people_from_yolo(image_path)\n","            \n","            # Store the counts\n","            actual_counts.append(actual_count)\n","            predicted_counts.append(predicted_count)\n","            \n","            print(f\"{filename}: Actual = {actual_count}, Predicted = {predicted_count}\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T12:50:42.112086Z","iopub.status.busy":"2024-08-17T12:50:42.111649Z","iopub.status.idle":"2024-08-17T12:50:42.118813Z","shell.execute_reply":"2024-08-17T12:50:42.117852Z","shell.execute_reply.started":"2024-08-17T12:50:42.112051Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["R¬≤: 0.9530484179401887\n","MAE: 0.6290322580645161\n"]}],"source":["# Evaluate the Testing \n","from sklearn.metrics import r2_score, mean_absolute_error\n","\n","r2 = r2_score(actual_counts, predicted_counts)\n","mae = mean_absolute_error(actual_counts, predicted_counts)\n","\n","print(f'R¬≤: {r2}')\n","print(f'MAE: {mae}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5497841,"sourceId":9109302,"sourceType":"datasetVersion"},{"datasetId":5514390,"sourceId":9132836,"sourceType":"datasetVersion"},{"datasetId":5555894,"sourceId":9190692,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
